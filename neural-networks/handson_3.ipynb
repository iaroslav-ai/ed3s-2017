{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit - learn interface and bayesian optimization\n",
    "\n",
    "It is often necessary to tune parameters of your deep neural network in order to achieve the best performance, such as number of layers, number of neurons in layer, etc. Sklearn interfaces allow to check different parameters easily. `BayesSearchCV` class from `scikit-optimize` allows to search for the parameters efficient in number of evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/.local/lib/python3.5/site-packages/sklearn/model_selection/_split.py:2010: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n",
      "/home/user/.local/lib/python3.5/site-packages/sklearn/utils/validation.py:444: DataConversionWarning: Data with input dtype uint8 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "[CV] model__neuron_type=elu, model__n_neurons=60, model__learning_rate=0.0444350660289 \n",
      "Epoch 1/5\n",
      "33750/33750 [==============================] - 0s - loss: 1.3619 - acc: 0.8629     \n",
      "Epoch 2/5\n",
      "33750/33750 [==============================] - 0s - loss: 1.4469 - acc: 0.8943     \n",
      "Epoch 3/5\n",
      "33750/33750 [==============================] - 0s - loss: 1.5124 - acc: 0.8991     \n",
      "Epoch 4/5\n",
      "33750/33750 [==============================] - 0s - loss: 1.5395 - acc: 0.9000     \n",
      "Epoch 5/5\n",
      "33750/33750 [==============================] - 0s - loss: 1.6007 - acc: 0.8973     \n",
      "[CV]  model__neuron_type=elu, model__n_neurons=60, model__learning_rate=0.0444350660289, score=0.8906666666666667, total=   4.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    4.9s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    4.9s finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/.local/lib/python3.5/site-packages/sklearn/utils/validation.py:444: DataConversionWarning: Data with input dtype uint8 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "45000/45000 [==============================] - 0s - loss: 1.3567 - acc: 0.8718     \n",
      "Epoch 2/5\n",
      "45000/45000 [==============================] - 0s - loss: 1.5966 - acc: 0.8901     \n",
      "Epoch 3/5\n",
      "45000/45000 [==============================] - 0s - loss: 1.6485 - acc: 0.8932     \n",
      "Epoch 4/5\n",
      "45000/45000 [==============================] - 0s - loss: 1.5827 - acc: 0.8986     \n",
      "Epoch 5/5\n",
      "45000/45000 [==============================] - 0s - loss: 1.6526 - acc: 0.8954     \n"
     ]
    }
   ],
   "source": [
    "import pickle as pc\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout\n",
    "from keras.layers.advanced_activations import LeakyReLU, ELU\n",
    "from keras.optimizers import Adam\n",
    "from keras.datasets import cifar10, mnist\n",
    "\n",
    "from misc import make_keras_picklable\n",
    "\n",
    "# necessary magic for pickling to work\n",
    "make_keras_picklable()\n",
    "\n",
    "# example implementation of sklearn estimator with keras\n",
    "class DNNClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, n_neurons=256, epochs=1, batch_size=256, \n",
    "                 learning_rate=1e-3, beta_1=0.9, beta_2=0.999,\n",
    "                neuron_type='relu'):\n",
    "\n",
    "        self.n_neurons = n_neurons\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "        self.neuron_type= neuron_type\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        # define network architecture\n",
    "        x = Input(shape=X[0].shape)\n",
    "        h = x\n",
    "        h = Dense(self.n_neurons)(h)\n",
    "        \n",
    "        if self.neuron_type == 'relu':\n",
    "            h = LeakyReLU()(h)\n",
    "        else:\n",
    "            h = ELU()(h)\n",
    "            \n",
    "        h = Dense(10, activation='softmax')(h)\n",
    "\n",
    "        self.model = Model(inputs=x, outputs=h)\n",
    "\n",
    "        # compile computational graph of NN\n",
    "        self.model.compile(loss='sparse_categorical_crossentropy',\n",
    "                      optimizer=Adam(\n",
    "                          lr=self.learning_rate,\n",
    "                          beta_1=self.beta_1,\n",
    "                          beta_2=self.beta_2,\n",
    "                      ),\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "        # train NN\n",
    "        self.model.fit(X, y,\n",
    "                        batch_size=self.batch_size,\n",
    "                        epochs=self.epochs,\n",
    "                        verbose=1)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.argmax(self.model.predict(X), axis=-1)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from skopt import BayesSearchCV\n",
    "\n",
    "X, y = mnist.load_data()[0]\n",
    "\n",
    "# reshape images to a vector\n",
    "X = np.reshape(X, (len(X), -1))\n",
    "\n",
    "# split data into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.75)\n",
    "\n",
    "# select indicies for training and validation folds\n",
    "I = range(len(X_train))\n",
    "I_train, I_val = train_test_split(I, train_size=0.75)\n",
    "\n",
    "# simple model pipeline declaration\n",
    "pipe = Pipeline([\n",
    "    ('scale', StandardScaler()),\n",
    "    ('model', DNNClassifier(epochs=5))\n",
    "])\n",
    "\n",
    "# Bayesian Optimization class \n",
    "# Can be run on cluster of machines with Dask\n",
    "model = BayesSearchCV(\n",
    "    estimator=pipe,\n",
    "    search_spaces={ # parameter search space\n",
    "        'model__n_neurons': (32, 512),\n",
    "        'model__learning_rate': (1e-3, 1e-1, 'log-uniform'),\n",
    "        'model__neuron_type': ['relu', 'elu'],\n",
    "    },\n",
    "    error_score=0.0,\n",
    "    cv=[[I_train, I_val]],\n",
    "    n_iter=3,\n",
    "    verbose=100000,\n",
    "    refit=False\n",
    ")\n",
    "\n",
    "# Run the search for best hyperparameters\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Fit the model with best parameters\n",
    "model.refit=True\n",
    "model._fit_best_model(X_train, y_train)\n",
    "\n",
    "pc.dump(model, open('model.bin', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's best parameters:\n",
      "{'model__neuron_type': 'relu', 'model__learning_rate': 0.0099561404859129319, 'model__n_neurons': 256}\n",
      "Model's test score:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/.local/lib/python3.5/site-packages/sklearn/utils/validation.py:444: DataConversionWarning: Data with input dtype uint8 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.938466666667\n",
      "Example estimations:\n",
      "[7 6 3]\n",
      "[7 6 3]\n"
     ]
    }
   ],
   "source": [
    "model = pc.load(open('model.bin', 'rb'))\n",
    "\n",
    "print(\"Model's best parameters:\")\n",
    "print(model.best_params_)\n",
    "\n",
    "print(\"Model's test score:\")\n",
    "print(model.score(X_test, y_test))\n",
    "\n",
    "print(\"Example estimations:\")\n",
    "print(model.predict(X_test[:3]))\n",
    "print(y_test[:3])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
